{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "author": "mes$",
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": false,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "latex_metadata": {
      "title": "PyTorch Foundations"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Chapter_1_Pytorch_Mechanics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushyag1/DeepLearningClass/blob/master/Chapter_1_Pytorch_Mechanics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24FD6L0-DDV",
        "colab_type": "text"
      },
      "source": [
        "Deep Learning, also called neural networks, or artificial neural networks, has become all the rage in recent years. Its lead to dramatic advances in the quality, accuracy, and usability machine learning. Technology that was considered impossible just 10 years ago is now widely deployed or considered technically possible, even if work remains. \n",
        "Digital assistants like Cortona, Google, Alex and Siri are ubiquitous and can react to natural spoken language. Self driving cars have been racking up millions of miles on the road as they are refined for eventual deployment. We can finally catalog and calculate just _how much_ of the Internet is made of cat photos. Deep Learning has been instrumental to the success of all of these use cases, and naturally is drawing a lot more attention from the public at large. In this book I will give you a wide breadth of exposure to the techniques commonly used in deep learning today, and the skills to start applying them to your own problems or to download new data and hone your skills. We are not going to go deep on the theory or derivations of deep learning, instead we will walk through the results, the intuition about them, and how to use them. \n",
        "\n",
        "That said, this book is _not_ a cook book of code snippets to just throw at any new problem. We will talk about some very advanced techniques, and build toward them in an incremental fashion. It is expected that you know and are familiar with programming in python, and have some passing memory of a calculus and a statistics course. You should also have taken at least one class on machine learning (ML). Topics from ML broadly will be quickly re-introduced, but our goal is to quickly move into new and interesting details about deep learning. \n",
        "\n",
        "Since I want you to develop a deeper understanding of deep learning, we are not going to start building any neural networks _just_ yet. For this first chapter, we are going to learn some foundations. The PyTorch library will be used throughout the book, and it has a particular approach to its design. Understanding the design will help remove some of the mystery about how deep learning is done, and how we can build and extend upon what we learn. \n",
        "\n",
        "We are also going to try and intermingle mathematically notation and code together, so that you can start to map equations to code. The goal is that you should eventually learn to read the mathematical notation or code in the same way you might read a paragraph. Any text you read `in this font indicates a mapping to code`. We try to give the variables in each code example useful names, as do the PyTorch developers. So when possible, we will use a code reference as the name we intend. For example, if we had an object that represented a sandwich, we might write out \"You can construct a `sandwich` from `bread` and `cheese`\" to indicate both the directions, but also map from English directly to the code examples we will write. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCchIy5F-DDX",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started with Colab\n",
        "\n",
        "We will be using Graphics Processing Units (GPUs) for _everything_ we do with deep learning. It is unfortunately a computationally demanding practice, and GPUs are essentially a _requirement_ for getting started, and especially when you start to work on larger applications. I use deep learning all the time as part of my day-job, and regularly kick off jobs that take a few _days_ to train on multiple GPUs. Some of my research experiments take as much as _a month_ of compute for each run! \n",
        "\n",
        "Unfortunately, GPUs are also decently expensive. The best option currently for most people who want to get started with deep learning is to spend \\$600-\\$1200 on one of the higher end Nvidia GTX or Titan GPUs. That is _if_ you have a computer that can be expanded/upgraded with a high end GPU. If not, you are probably looking at at least \\$1500-\\$2500 to build a nice workstation to put those GPUs in. Thats a pretty steep cost just to _learn_ about deep learning!\n",
        "\n",
        "As such, I've tired to make it so that you can run _every_ experiment and exercise problem in this book using just Google's Colab ( https://colab.research.google.com ). Colab is a service provided by Google, built on top of Jupyter notebooks, that provides an online python compute environment. As a part of Colab, you can also get temporary access to a GPU _for free_ ! This way you can get started learning about deep learning without such an expensive up-front cost. It even has PyTorch, and most of the libraries we will want to use, already installed and ready to go. Colab does not guarantee you a GPU, and eventually will put you in a low priority queue if you are using an excessive amount of GPU compute. If this happens, you may be allocated older/slower GPUs, or get an error message letting you know that no GPUs are available. If you just wait a few hours (or potentially a few days, depending on your usage and colab's demand), you should be able to access it again. \n",
        "\n",
        "While not required, I would encourage you to sign up for Colab Pro to make your life easier. Its available here : https://colab.research.google.com/signup , and does not fundamentally change how Colab works in any way. It simply gives you a higher priority for GPU access, and longer runtimes when using Colab. Colab pro is only \\$10 a month, so a pretty good deal for portable GPU access and still far cheaper than diving into new hardware. In my experience teaching a course, grading homeworks, and helping students with their projects, I found Colab Pro meant that I always got a GPU without issue. \n",
        "\n",
        "To access a GPU with Colab, you should click the \"Runtime\" menu in the top of the webpage. You will then want to click on the \"Change runtime type\" option near the bottom. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1KvMoed6Gco8EJCGHc11M6smt0aVYhcIx)\n",
        "\n",
        "\n",
        "One selected you should see the window below. The default type of runtime is \"None\". You want to instead select \"GPU\". \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1A_EoigbHo2wnfkCsMvCTGvGDtM5FEdKh)\n",
        "\n",
        "After hitting the \"Save\" button, you should now have access to an Nvidia GPU in your Colab session! You can double-check this by running the below command. \n",
        "\n",
        "```\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "The \"!\" is a specicial function of a Jupyter notebook. Instead of running python code, it will instead run the code on the command line of the host computer. `nvidia-smi` is a program that gives you information about all the GPUs running in your computer, and their current utilization. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1MH0uXdDuoy9hRrPgtv8T1YdTLwZSn2Ev)\n",
        "\n",
        "When I ran this command, I got the above output indicating a Tesla P4 with 7.6 GB of RAM was allocated for my use. You may get different results when you run this, and thats OK. Not having exact control is the price we pay for a free GPU! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO-uSLlj-DDX",
        "colab_type": "text"
      },
      "source": [
        "Thats all that was needed for us to get ready to run some code with Colab. It also comes with most machine learning libraries pre-installed and ready to go. For example, we will regularly make use of `seaborn` and `matplotlib` for easy plotting / visualization of our results, and NumPy for our initial data loading and working with arrays in general. Pandas is another one we will use to make it simple to look at and examine our results. The `tqdm` library is another useful utility that will provide easy access to progress bars with an estimated completion time, like this:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=19Qodawk-HK7Me3K-FxlNhjCSRbi8OAqx)\n",
        "\n",
        "We can simply import these libraries and they are ready to go! All of these are common tools in a machine learning practitioner's tool belt, and you should have at least some familiarity with them before diving into this book. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:24.594119Z",
          "start_time": "2020-05-09T05:50:24.019015Z"
        },
        "tags": [
          "remove_output"
        ],
        "id": "W8Q1HVI8-DDY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "dd927287-caff-4b37-8c1a-ddccb63dca5f"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.autonotebook import tqdm\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP20bNCI-DDc",
        "colab_type": "text"
      },
      "source": [
        "As we progress through this book, we are not going to repeatedly show all of the imports, as that is mostly a waste of paper. Instead they will be available on-line as part of the downloadable copies of the code, which can be found LINK GOES HERE ONCE READY. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:24.599061Z",
          "start_time": "2020-05-09T05:50:24.595465Z"
        },
        "tags": [
          "remove_cell"
        ],
        "id": "vg_vdSiE-DDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('png', 'pdf')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC1hLi2U-DDf",
        "colab_type": "text"
      },
      "source": [
        "# What is PyTorch?\n",
        "\n",
        "Before we get into neural networks & deep learning, lets talk about PyTorch a bit first. In particular, what is it? Obviously, it is a a library for deep learning, as thats the whole point of this book. But what are the mechanics and tools PyTorch gives us? We want to understand these foundations first, as everything we do will build upon them.\n",
        "\n",
        "We will begin by importing the torch library, and discussing _tensors_. Numpy and PyTorch are very similar in this regard. Both allow us to create n-dimensional arrays. A 0-dimensional array is called a _scalar_, and is any single number (e.g., \"3.4123\"). A 1-dimensional array is a _vector_ (e.g., [1.9, 2.6, 3.1, 4.0, 5.5]), and a 2-dimensional array is a _matrix_. Scalars, vectors, and matrices are all tensors. In fact, any value of $n$ is still a tensor! The word tensor is simply referring to the overall concept of an $n$-dimensional array. \n",
        "\n",
        "We care about tensors because they are a convenient way for us to organize much of our data, and our algorithms. This is the first foundation that PyTorch provides us, and we will often convert numpy to PyTorch tensors. So our first step, we will create four tensors matching the shapes below. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10kow0FLR8QgtyVjGOD_Do9UyAncAbTtv \"Tensors\")\n",
        "\n",
        "We will use some common notation to associate math symbols with tensors of a specific shape. We will use a capitol letter like $X$ or $Q$ to represent a tensor with two or more dimensions. If we are talking about a vector, we will use a lower case bold letter like $\\boldsymbol{x}$ or $\\boldsymbol{h}$. Last, we will use a lower case non-bold letter like $x$ or $h$ for scalars. \n",
        "\n",
        "In talking about and implementing neural networks, we often want to refer to a row within a larger matrix, or a scalar within a larger vector. So if we have a matrix $X$, we could use $\\boldsymbol{x_i}$ to reference the $i$'th row of $X$. In code, that would be `x_i = X[i,:]`. If we wanted the $i$'th row and $j$'th column, that would become $x_{i,j}$, which is not bold because it is reference a single value - making it a scalar. Again, the code version of that would be `x_ij = X[i,j]`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-dnVJns-DDg",
        "colab_type": "text"
      },
      "source": [
        "To use PyTorch we will need to import it as the `torch` package. With it, we can immediately start creating some tensors! Every time you nest a list within another list, you are creating a new dimension to the tensor that PyTorch will produce. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:25.711534Z",
          "start_time": "2020-05-09T05:50:25.485281Z"
        },
        "id": "QOLXJ1k8-DDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:25.718462Z",
          "start_time": "2020-05-09T05:50:25.713398Z"
        },
        "id": "Ow6jpi5--DDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch_scalar = torch.tensor(3.14)\n",
        "torch_vector = torch.tensor([1, 2, 3, 4])\n",
        "torch_matrix = torch.tensor([[1, 2,],\n",
        "                             [3, 4,],\n",
        "                             [5, 6,], \n",
        "                             [7, 8,]])\n",
        "#You don't have to format it like I \n",
        "#did, thats just for clarity! \n",
        "torch_tensor3d = torch.tensor([\n",
        "                            [\n",
        "                            [ 1,  2,  3], \n",
        "                            [ 4,  5,  6],\n",
        "                            ],\n",
        "                            [\n",
        "                            [ 7,  8,  9], \n",
        "                            [10, 11, 12],\n",
        "                            ],\n",
        "                            [\n",
        "                            [13, 14, 15], \n",
        "                            [16, 17, 18],\n",
        "                            ],\n",
        "                            [\n",
        "                            [19, 20, 21], \n",
        "                            [22, 23, 24],\n",
        "                            ]\n",
        "                              ])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEx5Fxu9-DDn",
        "colab_type": "text"
      },
      "source": [
        "And if we print the shapes of these tensors out, you should see that you get the same shapes listed above. Again, while scalars, vectors, and matrices are different things, they are all unified under the larger umbrella of \"tensors\". We care about this because we will use tensors of difference shapes to represent different types of data. We will get to those details later, for now are are going to simply focus on the mechanics PyTorch provides to work with tensors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:26.795353Z",
          "start_time": "2020-05-09T05:50:26.789921Z"
        },
        "id": "WTpiNicf-DDo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "11c13f31-0c82-4048-9315-e56e19cd4d63"
      },
      "source": [
        "print(torch_scalar.shape)\n",
        "print(torch_vector.shape)\n",
        "print(torch_matrix.shape)\n",
        "print(torch_tensor3d.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([])\n",
            "torch.Size([4])\n",
            "torch.Size([4, 2])\n",
            "torch.Size([4, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbGg_Kp3-DDs",
        "colab_type": "text"
      },
      "source": [
        "If you have done any machine learning or scientific computing in python, you have probably used the NumPy library. As you would expect, PyTorch supports converting these NumPy objects into their PyTorch counterparts. Since both of them represent data as tensors, this is a very painless process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:27.242985Z",
          "start_time": "2020-05-09T05:50:27.237049Z"
        },
        "id": "THZhIF_V-DDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "aeb1cac0-9c46-4256-840d-2f2fc7ccb17e"
      },
      "source": [
        "x_np = np.random.random((4,4))\n",
        "print(x_np)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.22469741 0.3267468  0.51034486 0.68320803]\n",
            " [0.40921799 0.74171087 0.31317534 0.08583361]\n",
            " [0.36255859 0.65426418 0.3306036  0.64279756]\n",
            " [0.51039383 0.27122545 0.69552935 0.73172842]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:27.298491Z",
          "start_time": "2020-05-09T05:50:27.269869Z"
        },
        "id": "W30QPqOF-DDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "4b875534-5af3-491d-d132-e5c877c1a4ab"
      },
      "source": [
        "x_pt = torch.tensor(x_np)\n",
        "print(x_pt)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2247, 0.3267, 0.5103, 0.6832],\n",
            "        [0.4092, 0.7417, 0.3132, 0.0858],\n",
            "        [0.3626, 0.6543, 0.3306, 0.6428],\n",
            "        [0.5104, 0.2712, 0.6955, 0.7317]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BijUNceS-DDy",
        "colab_type": "text"
      },
      "source": [
        "Both numpy and torch support multiple different data types. By default, numpy will use 64-bit floats, and PyTorch will default to 32-bit floats. However, if you create a PyTorch tensor from a numpy one, it will use the same type as the given numpy tensor. You can see that above where PyTorch suddenly felt the need to let us know that `dtype=torch.float64`, since it is not the default choice. \n",
        "\n",
        "For deep learning, the most common types we will care about are 32-bit floats, 64-bit integers (Longs), and booleans (i.e., binary \"True\"/\"False\"). Most operations will leave the type of a tensor unchanged, unless we explicitly create or cast it to a new version. To avoid issues with types, you can always specify explicitly what type of tensor you want to create when calling a function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:28.018451Z",
          "start_time": "2020-05-09T05:50:28.013061Z"
        },
        "id": "bjvgeR85-DDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "982d3efb-2726-42f6-c634-bc2a204a91d1"
      },
      "source": [
        "print(x_np.dtype, x_pt.dtype)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float64 torch.float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:28.053644Z",
          "start_time": "2020-05-09T05:50:28.047314Z"
        },
        "id": "d8vxWull-DD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9d085d5-63e7-45aa-8553-2e08b68f2d3d"
      },
      "source": [
        "x_np = np.asarray(x_np, dtype=np.float32)\n",
        "x_pt = torch.tensor(x_np, dtype=torch.float32)\n",
        "print(x_np.dtype, x_pt.dtype)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float32 torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrvbA1Jm-DD3",
        "colab_type": "text"
      },
      "source": [
        "The main exception to this is logic operations, which we can use to quickly create binary masks. So lets say we wanted to find every value in a tensor that was greater than 0.5. Both PyTorch and NumPy would allow us to use the standard logic operators to check for things like this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:28.710916Z",
          "start_time": "2020-05-09T05:50:28.704856Z"
        },
        "id": "X-SWHQKb-DD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b09344e3-9363-45cc-864e-b394fb14bc0a"
      },
      "source": [
        "b_np = (x_np > 0.5)\n",
        "print(b_np)\n",
        "print(b_np.dtype)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[False False  True  True]\n",
            " [False  True False False]\n",
            " [False  True False  True]\n",
            " [ True False  True  True]]\n",
            "bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:28.794979Z",
          "start_time": "2020-05-09T05:50:28.789051Z"
        },
        "id": "freB3LML-DD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3fbea9c5-1051-4d33-eae5-d4384b3e438d"
      },
      "source": [
        "b_pt = (x_pt > 0.5)\n",
        "print(b_pt)\n",
        "print(b_pt.dtype)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[False, False,  True,  True],\n",
            "        [False,  True, False, False],\n",
            "        [False,  True, False,  True],\n",
            "        [ True, False,  True,  True]])\n",
            "torch.bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wn2kRY0-DD8",
        "colab_type": "text"
      },
      "source": [
        "While the APIs between NumPy and PyTorch are not identical, they share many functions with the same names, behaviors, and characteristics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:29.585846Z",
          "start_time": "2020-05-09T05:50:29.577386Z"
        },
        "id": "sdLbM82p-DD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9d6c08c-9f7a-4b90-98ef-43a532a5c820"
      },
      "source": [
        "np.sum(x_np)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.4940357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:29.631270Z",
          "start_time": "2020-05-09T05:50:29.625507Z"
        },
        "id": "pjrrOadE-DD-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a428d64-81fa-45ff-8ad3-5615e99eb8bf"
      },
      "source": [
        "torch.sum(x_pt)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.4940)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrwWOdmD-DEA",
        "colab_type": "text"
      },
      "source": [
        "While many are the same, some are not _quite_ identical. There may be slight differences in behavior, or in the arguments required. These discrepancies are usually because the PyTorch version has made some changes that are particular to how these methods will be used for neural network design and execution. Below is an example of the transpose function, where PyTorch requires us to specific which two dimensions are to be transposed. NumPy simply takes the two dimensions and transposes them without complaint.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GM9AEYM9EVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6cb05cf0-d2f7-4edc-bce4-96917e224fb8"
      },
      "source": [
        "x_np"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22469741, 0.3267468 , 0.51034486, 0.68320805],\n",
              "       [0.40921798, 0.7417109 , 0.31317535, 0.08583362],\n",
              "       [0.3625586 , 0.65426415, 0.3306036 , 0.6427975 ],\n",
              "       [0.5103938 , 0.27122545, 0.69552934, 0.73172843]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:30.321789Z",
          "start_time": "2020-05-09T05:50:30.315115Z"
        },
        "id": "6Hl3OBry-DEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "ac6b4135-1b17-4100-dd31-dff0d2c8dcc1"
      },
      "source": [
        "np.transpose(x_np)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22469741, 0.40921798, 0.3625586 , 0.5103938 ],\n",
              "       [0.3267468 , 0.7417109 , 0.65426415, 0.27122545],\n",
              "       [0.51034486, 0.31317535, 0.3306036 , 0.69552934],\n",
              "       [0.68320805, 0.08583362, 0.6427975 , 0.73172843]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:30.373120Z",
          "start_time": "2020-05-09T05:50:30.365575Z"
        },
        "id": "Sji7WRGP-DED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "94c40faf-8c3b-4994-82c0-68e3b3d10ca7"
      },
      "source": [
        "torch.transpose(x_pt, 1, 2) # And this would transpose a \"batch\" of multiple matricies all at once"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0e325ab9c87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# And this would transpose a \"batch\" of multiple matricies all at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mMZ8QVb-DEF",
        "colab_type": "text"
      },
      "source": [
        "In this case, PyTorch does this because we often want to transpose different dimensions of the tensor for deep learning applications, where NumPy tries to stay with more general expectations. See below how we can \"transpose\" two of the dimensions in our `torch_tensor3d` from the start of the chapter. Originally it had a shape of $(4, 2, 3)$. If we transpose the first and third dimensions, we will get a shape of $(3, 2, 4)$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:31.213092Z",
          "start_time": "2020-05-09T05:50:31.207543Z"
        },
        "id": "fKM29P9c-DEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.transpose(torch_tensor3d, 0, 2).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFHUI9px-DEI",
        "colab_type": "text"
      },
      "source": [
        "Because such changes exist, you should always double check the PyTorch documentation at [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html) if you attempt to use a function you are familiar with, but suddenly find it is not behaving as expected! Its also a good tool to generally have open when using PyTorch. There are a lot of different functions that can help you within PyTorch, and we simply can not review them all! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-9Z2TkZ-DEI",
        "colab_type": "text"
      },
      "source": [
        "## GPU Acceleration\n",
        "\n",
        "The first important functionality that PyTorch gives us beyond what numpy can do is use a _Graphics Processing Unit_ (GPU) to accelerate mathematical calculations. \n",
        "\n",
        "### What is a GPU?\n",
        "\n",
        "What are GPUs? They are hardware in your computer that is specifically designed for doing 2D and 3D graphics, mainly to accelerate videos (watching an HD movie) or play video grames. So what does that have to do with neural networks? Well, a lot of the math involved in making 2D and 3D graphics fast is tensor based, or at least related. For this reason, GPUs have been getting good at doing a lot of the things we want very quickly. As graphics, and thus GPUs, got better and more powerful, people realized they could also be used for a lot of scientific computing and machine learning! \n",
        "\n",
        "So at a high level, you can think of GPUs as giant tensor calculators. You should almost always use a GPU when doing anything with neural networks. Its a good pair up since neural networks are very compute intensive, and GPUs are fast at the exact type of computations we need to perform. If you want to do deep learning in a professional context, you should invest in a computer with a powerful Nvidia GPU. But for now, we can get by for _free_ using Colab! \n",
        "\n",
        "The trick to using GPUs effectively is to avoid computing on a _small_ amount of data. This is because your computer's CPU must first move data to the GPU, then ask the GPU to perform it's math, wait for the GPU to finish, and then copy the results back off the GPU. The steps surrounding this process are fairly slow, and take longer than it would for the CPU to do the math itself if we are only calculating a few things. \n",
        "\n",
        "What exactly counts as 'too small'? Well, that depends on your CPU, GPU, and the math you are doing. If you are worried about this problem, you can do some benchmarking to see if the CPU is coming out faster. If so, you are probably working on too little data! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1RWV48j-DEJ",
        "colab_type": "text"
      },
      "source": [
        "Lets test some of that out right now with a matrix multiplication! This is a pretty basic linear algebra operation that is very common in neural networks. Whats the math for this? We have a matrix $X^{n,m}$ and $Y^{m,p}$, and we can compute a resulting matrix $C^{n, p} = X^{n,m} Y^{m,p}$. Note that $C$ has as many rows as $X$ and as many columns as $Y$. When implementing neural networks, we are going to be doing lots of operations that change the _shape_ of a tensor, which we can see happens when we multiply two matrices together. This will be a common source of bugs, so you should think about tensor shapes when writing code!\n",
        "\n",
        "Now, back to testing out these GPUs. Well use the _timeit_ library to help us, it allows us to run some code multiple times and tells us how long it took to run it. First we will make a larger matrix $X$, and we will compute $X X$ several times, and see how long that takes to run. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:36.087581Z",
          "start_time": "2020-05-09T05:50:32.557875Z"
        },
        "id": "YgtWFY_A-DEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import timeit\n",
        "x = torch.rand(2**11, 2**11)\n",
        "time_cpu = timeit.timeit(\"x@x\", globals=globals(), number=77)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnjIh6xnAULA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "round(time_cpu, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-08T04:06:13.180786Z",
          "start_time": "2020-05-08T04:06:13.174552Z"
        },
        "variables": {
          "round(time_cpu, 3)": "3.49"
        },
        "id": "CYBUb6uR-DEL",
        "colab_type": "text"
      },
      "source": [
        "You should see it takes a bit of time to run that code, but not too long. On my computer it took {{round(time_cpu, 3)}} seconds to run, which is stored in the `time_cpu` variable. Now how do we get PyTorch to use our GPU? First we need to create a `device` reference. We can ask PyTorch to give us one using the `torch.device` function. If you have an Nvidia GPU, and the CUDA drivers are installed properly, you should be able to pass in `\"cuda\"` as a string and get a object back representing that device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:36.124878Z",
          "start_time": "2020-05-09T05:50:36.092633Z"
        },
        "id": "jxESK0Xx-DEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Is CUDA available? :\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN6ynuJpCTle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK8arnKc-DEN",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a reference to the GPU (device) that we want to use, we just need to ask PyTorch to move that object to the given device. Luckily that can be done with a simple `to` function, and then we can use the same code as before! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:37.590819Z",
          "start_time": "2020-05-09T05:50:36.126069Z"
        },
        "id": "AHpsJA4Q-DEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = x.to(device)\n",
        "time_gpu = timeit.timeit(\"x@x\", globals=globals(), number=77)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poWVnrz8A1o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "round(time_gpu, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3BCDiYNA5ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_cpu/time_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgIwuby7BOll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_.cpu() # x_.to(torch.device(\"cpu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cbnv7QeAz5m",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "variables": {
          "round(time_cpu/time_gpu, 2)": "25.07",
          "round(time_gpu, 4)": "0.1392"
        },
        "id": "VAMgDFlk-DEP",
        "colab_type": "text"
      },
      "source": [
        "When I run this code, I get the time to perform 100 multiplications as {{round(time_gpu, 4)}} seconds, which is an instant {{round(time_cpu/time_gpu, 2)}}$\\times$ speedup! Now this was a pretty ideal case, as matrix multiplications are super efficient on GPUs, and we created a pretty big matrix. You should try making the matrix smaller and smaller and see how that impacts the speedup that you get. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbQcfsZM-DEP",
        "colab_type": "text"
      },
      "source": [
        "Something that we do need to be aware of is that this only works if every object involved is on the same device. Say you run the below code, where the variable `x` has been moved onto the GPU, and y has not (so it is on the CPU by default). \n",
        "\n",
        "```python\n",
        "x = torch.rand(128, 128).to(device)\n",
        "y = torch.rand(128, 128)\n",
        "x*y\n",
        "```\n",
        "\n",
        "You will end up getting an error message that says:\n",
        "```\n",
        "RuntimeError: expected device cuda:0 but got device cpu\n",
        "```\n",
        "The error will tell you which device the first variable is on (`\"cuda:0\"`) but that the second variable was on a different device ('cpu'). If we instead wrote `y*x` you would see the error  change too `\"expected device cpu but got device cuda:0\"` instead. Whenever you see an error like this, you have a bug somewhere that kept your from moving everything to the same compute device! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0auMiDpt-DEP",
        "colab_type": "text"
      },
      "source": [
        "The other thing that we want to be aware of is how to convert our PyTorch data back to the CPU. For example, we may want to convert a tensor back to a NumPy array so that we can pass it to Matplotlib, or save it to disk. The PyTorch `tensor` object as a `.numpy()` method that will do this for you, but if you call `x.numpy()` right now, you will get the error:\n",
        "\n",
        "```\n",
        "TypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu()\n",
        "to copy the tensor to host memory first.\n",
        "```\n",
        "Instead, you can use the handy shortcut function `.cpu()` to move an object back to the CPU, where we can interact with it in a normal way. So you will often see code that looks like `x.cpu().numpy()` when we want to access the results of our work. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2IwPQBG-DEQ",
        "colab_type": "text"
      },
      "source": [
        "THe `.to` and `.cpu()` methods make it easy to write code that is suddenly GPU accelerated. Once on a GPU or similar compute device, almost _every_ method that comes with PyTorch can be used and will net you a nice speedup! But sometimes we will want to store tensors and other PyTorch objects in a list, dictionary, or some other standard python collection. TO help us with that, we are going to define this `moveTo` function, which will recursevly go through the common python and PyTorch contains and move every device found onto the specified device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:37.787072Z",
          "start_time": "2020-05-09T05:50:37.599148Z"
        },
        "id": "75BuQezM-DEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moveTo(obj, device):\n",
        "    \"\"\"\n",
        "    obj: the python object to move to a device, or to move its contents to a device\n",
        "    device: the compute device to move objects to\n",
        "    \"\"\"\n",
        "    if isinstance(obj, list):\n",
        "        return [moveTo(x, device) for x in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(moveTo(list(obj), device))\n",
        "    elif isinstance(obj, set):\n",
        "        return set(moveTo(list(obj), device))\n",
        "    elif isinstance(obj, dict):\n",
        "        to_ret = dict()\n",
        "        for key, value in obj.items():\n",
        "            to_ret[moveTo(key, device)] = moveTo(value, device)\n",
        "        return to_ret\n",
        "    elif hasattr(obj, \"to\"):\n",
        "        return obj.to(device)\n",
        "    else:\n",
        "        return obj\n",
        "    \n",
        "some_tensors = [torch.tensor(1), torch.tensor(2)]\n",
        "print(some_tensors)\n",
        "print(moveTo(some_tensors, device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUUkm8_5EbmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vJSq5Kn-DET",
        "colab_type": "text"
      },
      "source": [
        "You should see above that the first time we printed the arrays we saw a `tensor(1)` and `tensor(2)`, but after using the `moveTo` function we saw `device='cuda:0'` show up! We won't have to use this function often, but when we do, it will make our code easier to read and write. With that, we now have the fundamentals to write some _fast_ code accelerated by GPUs! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O72gGMer-DEU",
        "colab_type": "text"
      },
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "So far what we've seen is that PyTorch provides an API similar to numpy for performing mathematical operations on tensors, with the advantage of using a GPU to perform faster math operations when available. The second major foundation that PyTorch gives us is called _automatic differentiation_. What this means is, as so long as we use PyTorch provided functions,  PyTorch can compute derivatives and gradients automatically for us! \n",
        "\n",
        "Now your first thought might be \"what is a gradient and why do I care about that?\". We care because we can use the derivative of a function $f(x)$ to help us find an input $x^*$ that is a _minimizer_ of $f(x)$. This means the value of $f(x^*)$ will be smaller than $f(x^* + y)$ for whatever value we set $y$ to. The mathy way to say this would be to state that $f(x^*) \\leq f(y),  \\forall x \\neq y$. \n",
        "\n",
        "$$f(\\underbrace{x^*}_{\\text{this specific value}}) \\overbrace{\\leq}^{\\text{is as small as or smaller}} f(y), \\underbrace{\\forall}_{\\text{for all cases where...}} \\underbrace{y \\neq x}_{y \\text{ is not equal to } x}$$\n",
        "\n",
        "Another way to say this is that if I wrote the below code, I would be stuck waiting for an infinite loop!\n",
        "```python\n",
        "while f(x_star) <= f(random.uniform(-1e100, 1e100)):\n",
        "    pass\n",
        "```\n",
        "\n",
        "Why do we want to minimize a function? For all the kinds of machine learning and deep learning we will discuss in this course, we will train neural networks by defining a _loss function_. The loss function tells the network, in a numeric and quantifiable way, how _badly_ it is doing at the problem right now. So if the loss is high, things are going poorly. A high loss means the network is \"losing the game\", and badly!  If the loss is zero, the network has perfectly solved the problem! We don't usually allow the loss to go negative, because that gets confusing to think about. \n",
        "\n",
        "When you read math about neural networks, you will often see the loss function defined as $\\ell(x)$, where $x$ are the inputs to the network, and $\\ell(x)$ gives us the loss the network received. Because of this, _loss functions return scalars_. This is important, because we can compare scalars and say that one is definitively bigger or smaller than another, so it becomes unambiguous as to how \"bad\" a network is at the \"game\". \n",
        "\n",
        "So we have stated that gradients are helpful, and perhaps you remember from a calculus class about minimizing functions using derivatives and gradients. Lets do a little bit of a math reminder about how we can find the minimum of a function using calculus. \n",
        "\n",
        "Say we have the function $f(x) = (x-2)^2$, lets define that with some PyTorch code and plot what the function looks like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:39.985746Z",
          "start_time": "2020-05-09T05:50:39.477526Z"
        },
        "id": "1WsZrV6P-DEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e6c9a2f9-abb8-457e-dfce-bd4f07eb3b7f"
      },
      "source": [
        "def f(x):\n",
        "    return torch.pow((x-2.0), 2) \n",
        "\n",
        "x_axis_vals = np.linspace(-7,9,100) \n",
        "y_axis_vals = f(torch.tensor(x_axis_vals)).numpy()\n",
        "\n",
        "sns.lineplot(x_axis_vals, y_axis_vals, label='$f(x)=(x-2)^2$')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0ca92752e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "application/pdf": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2GTtM_-VRmh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "481f681f-90a1-411c-ae48-9aa163a2c7bf"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-7., -6., -5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,\n",
              "        6.,  7.,  8.,  9.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "morsQX4C-DEY",
        "colab_type": "text"
      },
      "source": [
        "Ok, we can clearly see that the minimum of this function is at $x=2$, where we get the value of $f(2)=0$. But this is an intentionally easy problem, so lets say we couldn't plot it. We can use calculus to help us find the answer. \n",
        "\n",
        "We denote the derivative of $f(x)$ as $f'(x)$, and we can get the answer (using calculus) that $f'(x)=2 \\cdot x - 4$. The minimum of a function ($x^*$) exists at _critical points_, which are points where $f'(x) = 0$. So lets find them by solving for $x$. In our case we get:\n",
        "\n",
        "$$2 \\cdot x - 4 = 0$$\n",
        "$$2 \\cdot x = 4 $$ (add 4 to both sides)\n",
        "$$x = 4/2 = 2$$ (divide each side by 2)\n",
        "\n",
        "\n",
        "Now this required us to solve the above equation for when $f'(x)=0$. PyTorch can't quite do that for us, because we are going to be developing more complicated functions where finding the _exact_ answer is not possible. But say we have a current guess $x^?$, where we are pretty sure that it is not the minimizer. We can use $f'(x^?)$ to help us know how to adjust $x^?$ so that we move closer to a minimizer. \n",
        "\n",
        "\n",
        "How is that possible? Lets plot $f(x)$ and $f'(x)$ at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:41.214997Z",
          "start_time": "2020-05-09T05:50:40.848242Z"
        },
        "id": "qs1ySP6M-DEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "3d960ef4-acc1-4e85-a314-d1030f90b025"
      },
      "source": [
        "def fP(x): #Defining the derivative of f(x) manually\n",
        "    return 2*x-4\n",
        "\n",
        "y_axis_vals_p = fP(torch.tensor(x_axis_vals)).numpy()\n",
        "\n",
        "#First, lets draw a black line at 0, so that we can easily tell if something is positive or negative\n",
        "sns.lineplot(x_axis_vals, [0.0]*len(x_axis_vals), label=\"0\", color='black')\n",
        "sns.lineplot(x_axis_vals, y_axis_vals, label='$f(x) = (x-2)^2$')\n",
        "sns.lineplot(x_axis_vals, y_axis_vals_p, label=\"$f'(x)=2 x - 4$\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0ca93aa2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "application/pdf": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a9k5gTP-DEa",
        "colab_type": "text"
      },
      "source": [
        "Look at the orange line. You should notice that when we are too far left of the minimum ($x=2$), we see that $f'(x^?)<0$. When we are to the right of the minimum we instead get that $f'(x)>0$. Only when we are at a minimum, do we see that $f'(x^?) = 0$. So if $f'(x^?)<0$ we need to increase $x^?$ and if $f'(^?x)>0$ we need to decrease the value of $x^?$. The _sign_ of the gradient $f'$ tells us which _direction_ we should move to find a minimizer! \n",
        "\n",
        "We also care about the _magnitude_ of $f'(x^?)$. Because we are looking at a one dimensional function, the magnitude just means the absolute value of $f'(x^?)$, i.e., $|f'(x^?)|$. The magnitude gives us an idea about how far away we are from the minimizer. So the sign of $f'(x^?)$ (<0 or >0) tells us which _direction_ we should head, and the size ($|f'(x)|$) tells us how _far_ we should head. \n",
        "\n",
        "This is not a coincidence. This will _always_ be true for any function. So if we can compute a derivative, we can find a minimizer. Now, you may be thinking, \"I don't remember my calculus all that well...\" or complaining that I skipped the steps on how to compute $f'(x)$. This is precisely why we use PyTorch, automatic differentiation will compute the value of $f'(x)$ for us! Lets use this toy example of $f(x)=(x-2)^2$ to see how it works. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2W9Bu4z-DEa",
        "colab_type": "text"
      },
      "source": [
        "First, lets create a new variable that we want to minimize. We will do this similar to before, but we will add a new flag that tells PyTorch to start keeping track of the gradient. This gets stored in a variable called 'grad', which does not exist yet since we haven't actually computed anything. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:43.108254Z",
          "start_time": "2020-05-09T05:50:43.102273Z"
        },
        "id": "Q_oeYPDA-DEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f115aa4a-f119-47a3-8626-7905e5a24882"
      },
      "source": [
        "x = torch.tensor([-3.5], requires_grad=True)\n",
        "print(x.grad)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBS7_lKh-DEd",
        "colab_type": "text"
      },
      "source": [
        "Ok, so we see there is no current gradient. Lets try computing $f(x)$ though and see if anything changes now that we set `requires_grad=True`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:44.190853Z",
          "start_time": "2020-05-09T05:50:44.184519Z"
        },
        "id": "c_SULtJu-DEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63ff7ffc-85b6-418a-dd4d-4b89b38ad304"
      },
      "source": [
        "value = f(x)\n",
        "print(value)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([30.2500], grad_fn=<PowBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d28g449D-DEi",
        "colab_type": "text"
      },
      "source": [
        "Now when we print the value of the returned variable, we get a slightly different output. We see the first part where the value \"30.25\" is printed, which is the correct value of $f(-3.5)$. But we also see this new `grad_fn=<PowBackward0>`. Once we tell PyTorch to start calculating gradients, it will begin to keep track of _every_ computation we do. It uses this information to go backwards and calculate the gradients for everything that was used and had a `requires_grad` flag set to `True`. \n",
        "\n",
        "Once we have a single _scalar_ value, we can tell PyTorch to go back and use this information to actualy compute the gradients. THisis done using the `.backward()` function, after which we will see there is now a gradient in our original object!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:45.131154Z",
          "start_time": "2020-05-09T05:50:45.124008Z"
        },
        "id": "8bsbzLHN-DEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUnWlWQF-DEl",
        "colab_type": "text"
      },
      "source": [
        "That covers the mechanics of how PyTorch can compute gradients for us! Now we can use this automatic differentiation of our PyTorch function $f(x)$ to numerically find the answer that $f(2)=0$. FIrst we are going to describe it using a mathematically notation, and then again in actual code. \n",
        "\n",
        "We start with our current guess $x_{cur} = -3.5$. I've chosen 3.5 arbitrarily, in real life you would usually pick a random value.  We will also keep track of our previous guess using $x_{prev}$. Since we have not done anything yet, its fine to set the \"previous\" step to any large value (e.g., $x_{prev} = x_{cur}*100$). \n",
        "\n",
        "Next, we compare if our current and previous guesses are very similar. We do this by checking if $\\|x_{cur}-x_{prev}\\|_2 > \\epsilon$. The function $\\|z\\|_2$ is called the _norm_ or _2-norm_. Norms are the most common and standard ways of measuring _magnitude_ for vectors and matrices. For one dimensional cases (like this one!), the 2-norm is the exact same thing as the absolute value.  If we do not explicitly state what kind of norm we are talking about, you should always assume the 2-norm. The value $\\epsilon$ is a common mathematical notation to refer to some arbitrary small value. \n",
        "\n",
        "Ok, so now we know that $\\|x_{cur}-x_{prev}\\|_2 > \\epsilon$ is how we check if there are large ($> \\epsilon$) magnitude ($\\|\\cdot\\|_2$) changes ($x_{cur}-x_{prev}$) between our guesses. If this is false, that means $\\|x_{cur}-x_{prev}\\|_2 \\leq \\epsilon$, whichs means the change was small, and that we can stop. Once we stop, we accept $x_cur$ as our answer to the value of $x$ that minimized $f(x)$! If not, we need a new _better_ guess! \n",
        "\n",
        "To get this new guess, we are going to move in the _opposite_ direction of the derivative. This looks like: $x_{cur} = x_{cur} - \\eta \\cdot f'(x_{cur})$. The value $\\eta$ is called the _learning rate_ , and is usually a small value like $\\eta=0.1$ or $\\eta=0.01$. We do this because the gradient $f'(x)$ tells us which way to head, but only gives us a _relative_ answer about how far away we are. It doesn't tell us exactly how far we should travel in that direction. \n",
        "\n",
        "![step size](https://drive.google.com/uc?export=view&id=1vByqHbDFLxX36OEP49Dw8EXH9BMw5JYA)\n",
        "\n",
        "So we prefer to take smaller steps in the current direction so that we don't \"drive past\" the answer, and have to turn back around. Look at the above example of our function to understand how that happens. If we have the _exactly_ correct best value of $\\eta$ (middle image), we can take one step to the minimum! But we do not know what that value is. If we are instead conservative, and choose a value that is likely smaller than we need we may take more steps to get to the answer, but we will eventually get there (left image). But if we set our learning rate too high, we can end up shooting past the solution and bouncing around it instead (right image)!  \n",
        "\n",
        "\n",
        "That might sound like a lot of scary math, but hopefully you will feel better about it when you look at the code that does the work. Its only a few lines long! At the end of the loop, we print out the value of $x_{cur}$ and see that it is equal to 2.0, PyTorch found the answer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:46.612298Z",
          "start_time": "2020-05-09T05:50:46.587461Z"
        },
        "id": "c398hxLA-DEm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a47c7100-c6e3-4235-ff21-4bcd1be8de56"
      },
      "source": [
        "x = torch.tensor([-3.5], requires_grad=True)\n",
        "\n",
        "x_cur = x.clone()\n",
        "x_prev = x_cur*100\n",
        "epsilon = 1e-5\n",
        "eta = 0.1\n",
        "\n",
        "while torch.norm(x_cur-x_prev) > epsilon:\n",
        "    x_prev = x_cur.clone() #We need to make a clone here so that x_prev and x_cur don't point to the same object!\n",
        "    \n",
        "    #Compute our function, gradient, and update\n",
        "    value = f(x)\n",
        "    value.backward()\n",
        "    x.data -= eta * x.grad\n",
        "    x.grad.zero_() #We need to zero out the old gradient, as py-torch will not do that for us\n",
        "    \n",
        "    #What are we currently now?\n",
        "    x_cur = x.data\n",
        "    \n",
        "print(x_cur)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0w-ObUa-DEn",
        "colab_type": "text"
      },
      "source": [
        "# Optimizing Parameters\n",
        "\n",
        "What we just did, finding the minimum of a function $f(\\cdot)$, is called _optimization_. Because we will specify the goal of our network using a loss function $\\ell(\\cdot)$, we can optimize $f(\\cdot)$ to minimize our loss. If we reach a loss $\\ell(\\cdot) = 0$, that means our network (appears) to have perfectly solved the problem. This why why we care about optimization. This is foundational to how most modern neural networks are trained today. \n",
        "\n",
        "Because of how important optimization is, PyTorch includes some additional concepts to help us organize our code, and train a neural network. The first concept is that of a `Parameter`. A `Parameter` of a model is a value that we want to alter using optimization, so that we can try and reduce our loss $\\ell(\\cdot)$. We can easily convert any tensor into a `Parameter` using the `nn.Parameter` class. So to do that, lets re-solve the previous problem of minimizing $f(x) = (x-2)^2$ with an initial guess of $x_{cur} = 3.5$. The first thing we will do is create a `Parameter` object for the value of $x$, since that is what we are going to alter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:48.094805Z",
          "start_time": "2020-05-09T05:50:48.089754Z"
        },
        "id": "oONNI05v-DEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_param = torch.nn.Parameter(torch.tensor([-3.5]), requires_grad=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18cMD9fX-DEp",
        "colab_type": "text"
      },
      "source": [
        "The object `x_param` is now a `nn.Parameter`, which behaves the same way as Tensors do. We can use a `Parameter` anywhere that we would use a tensor in PyTorch and the code will work just fine. But now we can create an `Optimizer` object. The simplest optimizer we will use is called `SGD`, which stands for _Stochastic Gradient Descent_. The word \"gradient\" is there because we are using the gradients/derivatives of functions. \"Descent\" because we are minimizing or \"descending\" to a lower value of the function that we are minimized. We will get to the \"stochastic\" part in the next chapter! \n",
        "\n",
        "To use SGD, we simply need to create the associated object with a `list` of `Parameter`s that we want to adjust. We can also specify the learning rate $\\eta$, or accept the default. The below code we will specify eta to match the original code above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:49.576348Z",
          "start_time": "2020-05-09T05:50:49.571725Z"
        },
        "id": "7Jxaz6Vb-DEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.SGD([x_param], lr=eta)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaiOoi7X-DEr",
        "colab_type": "text"
      },
      "source": [
        "Now we can re-write the previous ugly loop into something cleaner, that looks much closer to how we will train neural networks in practice. We will loop over the optimization problem a fixed number of times, which we often call _epochs_. The `zero_grad` method does the cleanup we did manually before for _every_ parameter that was passed in as an input. We compute our \"loss\", call `.backward()` on that loss, and then ask the optimizer to perform one `.step()` of the optimization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:50:51.537416Z",
          "start_time": "2020-05-09T05:50:51.517767Z"
        },
        "tags": [
          "remove_output"
        ],
        "id": "tf4uEx8T-DEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9831ac7d-9355-4311-e931-fafbf4e774c7"
      },
      "source": [
        "for epoch in range(60):\n",
        "    optimizer.zero_grad() #x.grad.zero_()\n",
        "    loss_incurred  = f(x_param)\n",
        "    #f(inputs, parameters)\n",
        "    loss_incurred.backward()\n",
        "    optimizer.step() #x.data -= eta * x.grad\n",
        "print(x_param.data)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "variables": {
          "x_param.data[0]": "tensor(2.0000)"
        },
        "id": "4-Nl4Weu-DEt",
        "colab_type": "text"
      },
      "source": [
        "And now you should see the code prints out {{x_param.data[0]}}, just like before! This will make our lives easier when we have literally _millions_ of parameters in our network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A02fz2_v-DEt",
        "colab_type": "text"
      },
      "source": [
        "# Loading DataSet Objects\n",
        "\n",
        "Now we have learned a little bit about the basic tools PyTorch provides us. We want to start training a neural network to solve our problems. But first, we need some data! Using the common notation of machine learning, we need a set of input data $X$ and associated output labels $\\boldsymbol{y}$. In PyTorch we will represent that with a `Dataset` object.  Lets start by loading a familiar dataset from ScikitLearn, MNIST! We will convert it from a NumPy array to the form that PyTorch would like to see that data as. \n",
        "\n",
        "PyTorch uses a `Dataset` class to represent a dataset, and it encodes the information about:\n",
        "1) How many items are in the dataset?\n",
        "2) How to get the n'th item in the data set!\n",
        "So lets go ahead and look at what that looks like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:51:08.005470Z",
          "start_time": "2020-05-09T05:50:53.403873Z"
        },
        "id": "R7H_-anN-DEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ4eQ1hh-DEv",
        "colab_type": "text"
      },
      "source": [
        "We can see that we have loaded the classic MNIST dataset with a total of 70,000 rows and 784 features. Now we will crease a simple Dataset class that takes in `X, y` as input. We need to define a `__getitem__` method, which will return the data and label as a `tuple(inputs, outputs)`. The `inputs` are the objects we want to give to our model as inputs, and the `outputs` are used for the output. We also need to implement the `__len__` function that returns how large the dataset is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:51:08.012227Z",
          "start_time": "2020-05-09T05:51:08.007453Z"
        },
        "id": "d6kC-_zm-DEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleDataset(Dataset):\n",
        "        \n",
        "    def __init__(self, X, y):\n",
        "        super(SimpleDataset, self).__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        inputs = torch.tensor(self.X[index,:], dtype=torch.float32)\n",
        "        targets = torch.tensor(int(self.y[index]), dtype=torch.int64)\n",
        "        return inputs, targets \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "#Now we can make a PyTorch dataset! \n",
        "dataset = SimpleDataset(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxSRMw7q-DEx",
        "colab_type": "text"
      },
      "source": [
        "Now we have a simple dataset object! It keeps the entire dataset in memory, which is OK for small datasets, but something we will want to fix in the future. We can confirm that the dataset still has 70,000 examples, and each example as 784 features, just as before. We can quickly confirm that the length and index functions we implemented work as expected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:51:08.019806Z",
          "start_time": "2020-05-09T05:51:08.014375Z"
        },
        "id": "LTkwcieO-DEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Length: \", len(dataset))\n",
        "example, label = dataset[0]\n",
        "print(\"Features: \", example.shape) #Will return 784\n",
        "print(\"Label of index 0: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LFHJL88-DEz",
        "colab_type": "text"
      },
      "source": [
        "The MNIST dataset is a dataset of hand drawn numbers. We can visualize this by re-shaping the data back into an image, just to confirm our data loader is working. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:51:08.216798Z",
          "start_time": "2020-05-09T05:51:08.021457Z"
        },
        "max_h": 0.3,
        "max_w": 0.9,
        "id": "GpyIuJGV-DEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(example.reshape((28,28)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To3FtP7p-DE1",
        "colab_type": "text"
      },
      "source": [
        "## Creating a Training and Testing Split\n",
        "\n",
        "Now we have _all_ our data in one dataset! However, like good machine learning practitioners, we should create a training and a testing split. In some cases we have a dedicated training and testing dataset. If that is the case, you should create two separate `Dataset` objects, one for training and one for testing, from the respective data sources. \n",
        "\n",
        "In this case, we just have one dataset. PyTorch has a simple utility that can help us break the corpus into a train and test split. Lets say we wanted 20% of the data to be used for testing, that could be done as follows using the `random_split` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T05:51:08.226168Z",
          "start_time": "2020-05-09T05:51:08.218366Z"
        },
        "id": "Dg0ZSuBM-DE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = int(len(dataset)*0.8)\n",
        "test_size = len(dataset)-train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_size, test_size))\n",
        "print(\"{} examples for training and {} for testing\".format(len(train_dataset), len(test_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhItNLon-DE4",
        "colab_type": "text"
      },
      "source": [
        "And now we have a train and test set! In reality, the first 60,000 points are the standard training set for MNIST and the last 10,000 the standard test set. But the point was to show you the function for creating randomized splits yourself. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWPXJU6g-DE4",
        "colab_type": "text"
      },
      "source": [
        "With that, we have learned about all of the foundational tools that PyTorch provides. \n",
        "\n",
        "1. A NumPy like tensor API, which supports GPU acceleration. \n",
        "2. Automatic differentiation, which lets us solve optimization problems\n",
        "3. An abstraction for datasets. \n",
        "\n",
        "We will start building on this foundation, and you may notice it starts to impact how you think about neural networks in the future. They do not magically do what is asked, but they try to numerically solve a goal specified by a loss function  $\\ell(\\cdot)$. We need to make sure we are careful in how we define or choose $\\ell(\\cdot)$, because that will determine what the algorithm learns. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypOJZfq-DE4",
        "colab_type": "text"
      },
      "source": [
        "# HW 1, Due  September 14th, 2020, 1 minute before class starts. \n",
        "\n",
        "1. Write a series of `for` loops that compute the average value in `torch_tensor3d`\n",
        "1. Write code that indexes into `torch_tensor3d` and prints out the value \"13\". \n",
        "1. For every power of 2 (i.e., $2^i$ or `2**i` ) up to $2^{11}$, create a random matrix $X \\in \\mathbb{R}^{2^i, 2^i}$ (i.e., `X.shape` should give `(2**i, 2**i)`). Time how long it takes to compute $X X$ (i.e., `X @ X`) on a CPU and on a GPU and plot the speedup. For what sized matrices is the CPU faster than the GPU? \n",
        "2. We used PyTorch to find the numeric solution to $f(x) = (x-2)^2$. Write code that will find the solution to  $f(x)= \\sin(x-2)\\cdot(x+2)^2+\\sqrt{\\cos(x)}$. What answer do you get?\n",
        "5. Affirm that you will not share the coure notebooks or images with any other person and will only use them for personal use.  Acknowledged!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjiqRyWoKL-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch_tensor3d[1,1,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADZFG4ZSVnje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}