{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " DATA690 Fall 2020: HW#2 PushyamiReddyGinnavaram.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcNqT0FqpTFLgeeA9R1xGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushyag1/DeepLearningClass/blob/master/DATA690_Fall_2020_HW_2_PushyamiReddyGinnavaram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AImSLIKLNN7S"
      },
      "source": [
        "# Question 1:\n",
        "The input range of data can have a large impact on a neural network. This applies to inputs _and_ outputs, like for regression problems. Try applying  Scikit-learn's `StandardScaler` to the targets $\\boldsymbol{y}$ of the toy regression problem at the start of this chapter, and train a new neural network on it. Does changing the scale of the outputs help or hurt the model's predictions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e0lKqAWTkJU"
      },
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVpQlLvgoLP5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import * \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/MPDL Fall 2020/')\n",
        "\n",
        "from mpdl import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx0oGFKHop5z"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#importing StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIm93hW8nZFN"
      },
      "source": [
        "def train_simple_network(model, loss_func, training_loader, epochs=20, device=\"cpu\"):\n",
        "    #Yellow step is done here. We create the optimizer and move the model to the compute device\n",
        "    #SGD is Stochastic Gradient Decent over the parameters $\\Theta$\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "    #Place the model on the correct compute resource (CPU or GPU)\n",
        "    model.to(device)\n",
        "    #The next two for loops handle the Red steps, iterating through all the data (batches) multiple times (epochs)\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "    \n",
        "        model = model.train()#Put our model in training mode\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in tqdm(training_loader, desc=\"Batch\", leave=False):\n",
        "            #Move the batch of data to the device we are using. this is the last red step\n",
        "            inputs = moveTo(inputs, device)\n",
        "            labels = moveTo(labels, device)\n",
        "\n",
        "            #First a yellow step, prepare the optimizer. Most PyTorch code will do this first to make sure everything is in a clean and ready state. \n",
        "\n",
        "            #PyTorch stores gradients in a mutable data structure. So we need to set it to a clean state before we use it. \n",
        "            #Otherwise, it will have old information from a previous iteration\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #The next two lines of code perform the two blue steps\n",
        "            y_hat = model(inputs) #this just computed $f_\\theta(\\boldsymbol{x_i})$\n",
        "\n",
        "            # Compute loss.\n",
        "            loss = loss_func(y_hat, labels)\n",
        "\n",
        "            #Now the remaining two yellow steps, compute the gradient and \".step()\" the optimizer!\n",
        "            loss.backward()# $\\nabla_\\Theta$ just got computed by this one call!\n",
        "\n",
        "            #Now we just need to update all the parameters! \n",
        "            optimizer.step()# $\\Theta_{k+1} = \\Theta_k âˆ’ \\eta \\cdot \\nabla_\\Theta \\ell(\\hat{y}, y)$\n",
        "\n",
        "            #Now we are just grabbing some information we would like to have\n",
        "            running_loss += loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43k0k9n7qmBw"
      },
      "source": [
        "scalar = StandardScaler()\n",
        "\n",
        "#initializing StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkci1JIcqki-"
      },
      "source": [
        "#Create a 1-dimensional input\n",
        "X = np.linspace(0, 20, num=200)\n",
        "#create an output\n",
        "y = X + np.sin(X)*2 + np.random.normal(size=X.shape)\n",
        "sns.scatterplot(x=X, y=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_TLpm48rkgU"
      },
      "source": [
        "y1=y.reshape(-1,1)\n",
        "\n",
        "#reshaping the target variables y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHQt7Hp2sZxi"
      },
      "source": [
        "yscalar = scalar.fit_transform(y1)\n",
        "\n",
        "#scaling the variables and then fit the values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcuQi4wGsexN"
      },
      "source": [
        "class Simple1DRegressionDataset(Dataset):\n",
        "        \n",
        "    def __init__(self, X, yscalar):\n",
        "        super(Simple1DRegressionDataset, self).__init__()\n",
        "        self.X = X.reshape(-1,1)\n",
        "        self.yscalar = yscalar.reshape(-1,1)\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.X[index,:], dtype=torch.float32), torch.tensor(self.yscalar[index], dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    \n",
        "training_loader = DataLoader(Simple1DRegressionDataset(X, yscalar), shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbMfFikdsxz3"
      },
      "source": [
        "in_features = 1\n",
        "out_features = 1\n",
        "model = nn.Linear(in_features, out_features)\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFkGM5HefIIm"
      },
      "source": [
        "train_simple_network(model, loss_func, training_loader, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzQlmoE4E6co"
      },
      "source": [
        "with torch.no_grad():\n",
        "    Y_pred = model(torch.tensor(X.reshape(-1,1), device=device, dtype=torch.float32)).cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n_-A_pwd5m0"
      },
      "source": [
        "yscalar_reshape = yscalar.reshape(-1,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1AFqfEpIkCa"
      },
      "source": [
        "sns.scatterplot(x=X, y=yscalar_reshape, color='blue', label='Data') #The data\n",
        "sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Linear Model') #What our model learned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M8jctriq3s4"
      },
      "source": [
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1,  30),#hidden layer\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),#activation\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),#activation\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),#activation\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),#activation\n",
        "    nn.Linear(30, 1),#output layer\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri8N25Bxfbsc"
      },
      "source": [
        "loss_func = nn.MSELoss()\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "train_simple_network(model, loss_func, training_loader, device=device)\n",
        "\n",
        "#training the model with multiple hidden layers and activation layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhd-sNB2selq"
      },
      "source": [
        "with torch.no_grad():\n",
        "    Y_pred = model(torch.tensor(X.reshape(-1,1), device=device, dtype=torch.float32)).cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crVuswzjsinS"
      },
      "source": [
        "sns.scatterplot(x=X, y=yscalar_reshape, color='blue', label='Data') #The data\n",
        "sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Linear Model') #What our model learned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sd8zY_Amhhh"
      },
      "source": [
        "#### Does changing the scale of the outputs help or hurt the model's predictions?\n",
        "\n",
        "Yes, changing the scale of the outputs hurt the model's predictions, this makes the model and the training process unstable because the target variable i.e., y with a collection of values , can give us error in gradient value with the effective change in the weight values . But by adding hidden layers and activation functions it is giving a little non linearity but not that well trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teFblKz8kbuj"
      },
      "source": [
        "# Question 2:\n",
        " The AUC metric does not follow the standard pattern in scikit-learn, as it requires `y_pred` to be a vector of shape $(N)$ instead of a matrix of shape $(N, 2)$. Write a wrapper function for AUC that will make it compatible with our `train_simple_network` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f4E7DBOkl-e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "#from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "\n",
        "#importing required sklearn metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukj3WR0bB4In"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "#importing make moons dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4j0tvZfkmd4"
      },
      "source": [
        "def train_simple_network(model, loss_func, train_loader, val_loader=None, score_funcs=None, \n",
        "                         epochs=50, device=\"cpu\", checkpoint_file=None):\n",
        "    to_track = [\"epoch\", \"total time\", \"train loss\"]\n",
        "    if val_loader is not None:\n",
        "        to_track.append(\"val loss\")\n",
        "    for eval_score in score_funcs:\n",
        "        to_track.append(\"train \" + eval_score )\n",
        "        if val_loader is not None:\n",
        "            to_track.append(\"val \" + eval_score )\n",
        "        \n",
        "    total_train_time = 0 #How long have we spent in the training loop? \n",
        "    results = {}\n",
        "    #Initialize every item with an empty list\n",
        "    for item in to_track:\n",
        "        results[item] = []\n",
        "        \n",
        "    #SGD is Stochastic Gradient Decent.\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "    #Place the model on the correct compute resource (CPU or GPU)\n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "        model = model.train()#Put our model in training mode\n",
        "        \n",
        "        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix=\"train\", desc=\"Training\")\n",
        "\n",
        "        results[\"total time\"].append( total_train_time )\n",
        "        results[\"epoch\"].append( epoch )\n",
        "        \n",
        "        if val_loader is not None:\n",
        "            model = model.eval()\n",
        "            with torch.no_grad():\n",
        "                run_epoch(model, optimizer, val_loader, loss_func, device, results, score_funcs, prefix=\"val\", desc=\"Testing\")\n",
        "                    \n",
        "    if checkpoint_file is not None:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'results' : results\n",
        "            }, checkpoint_file)\n",
        "\n",
        "    return pd.DataFrame.from_dict(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvg-5cjtUZqd"
      },
      "source": [
        "\n",
        "def AUC_function(x, y):\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(x, y, pos_label=1)\n",
        "  return metrics.auc(fpr, tpr)\n",
        "\n",
        "#defining a function to calculate the AUC(Area Under the ROC Curve), AUC requires two values to be passed in the function because we need to return the values of tpr and fpr , so let us now initialize with x and y\n",
        "#fpr(false positive rate) and tpr(true positive rate)\n",
        "\n",
        "#reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuTLHvKeFyfi"
      },
      "source": [
        "def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix=\"\", desc=None):\n",
        "    running_loss = []\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    start = time.time()\n",
        "    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n",
        "        #Move the batch to the device we are using. \n",
        "        inputs = moveTo(inputs, device)\n",
        "        labels = moveTo(labels, device)\n",
        "\n",
        "        y_hat = model(inputs) #this just computed f_Î˜(x(i))\n",
        "        # Compute loss.\n",
        "        loss = loss_func(y_hat, labels)\n",
        "\n",
        "        if model.training:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        #Now we are just grabbing some information we would like to have\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n",
        "            #moving labels & predictions back to CPU for computing / storing predictions\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            y_hat = y_hat.detach().cpu().numpy()\n",
        "            #add to predictions so far\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(y_hat.tolist())\n",
        "    #end training epoch\n",
        "    end = time.time()\n",
        "    \n",
        "    y_pred = np.asarray(y_pred)\n",
        "    if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "    \n",
        "    \n",
        "    #metrics.auc(fpr, tpr)\n",
        "    #Else, we assume we are working on a regression problem\n",
        "    #print(roc_auc_score(y_true,y_pred)) # printing for reference\n",
        "    results[prefix + \" loss\"].append( np.mean(running_loss) )\n",
        "    for name, score_func in score_funcs.items(): #for the score functions \n",
        "      if name == \"AUC\":     #checking if the name is \"AUC\"\n",
        "        AUC_function(y_true,y_pred)  #passing y_true and y_pred to the AUC_function defined above, which returns the metric values\n",
        "        results[prefix + \" \" + name].append(AUC_function(y_true,y_pred))  #appending the values\n",
        "      else:\n",
        "        try:\n",
        "          #fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred) \n",
        "          results[prefix + \" \" + name].append( score_func(y_true, y_pred) )   #to display the remaining scores i.e., Accuracy and F1 scores\n",
        "        except:\n",
        "          results[prefix + \" \" + name].append(float(\"NaN\"))\n",
        "    return end-start #time spent on epoch\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR0ebAPeCAqb"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "#train_simple_network(model, loss_func, training_loader, epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP_tVACGBVTQ"
      },
      "source": [
        "X_train, y_train = make_moons(n_samples=8000, noise=0.4)\n",
        "X_test, y_test = make_moons(n_samples=200, noise=0.4)\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "training_loader = DataLoader(train_dataset, shuffle=True)\n",
        "testing_loader = DataLoader(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd2_frcpBiV6"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(2,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30, 2),\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaUKM1VeJnn4"
      },
      "source": [
        "results_pd = train_simple_network(model, loss_func, training_loader, epochs=10, \n",
        "                                  val_loader=testing_loader, \n",
        "                                  checkpoint_file='model.pt', \n",
        "                                  score_funcs={'Acc':accuracy_score,'F1': f1_score,'AUC': AUC_function}) \n",
        "\n",
        "#score_funcs for accuracy, f1 and AUC "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g59a3F5KjeY"
      },
      "source": [
        "\n",
        "results_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vQljdHpINgB"
      },
      "source": [
        "# Question 3: \n",
        "Write a new function `resume_simple_network`, which loads a `checkpoint_file` from disk, restores both the `optimizer` and `model` state, and continues training to a specified total number of epochs. So if the model was saved after 20 epochs, and you specify 30 epochs, it should only perform 10 more epochs of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a5C0Imo0oLm"
      },
      "source": [
        "doing Resume_simple_network for the above (question 2) executed model. In that the epochs were 10, here we are passing 20 epochs and showing the remaining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip6B3CVKhSxc"
      },
      "source": [
        "def resume_simple_network(epoch, checkpointfile, device='cpu'):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "  checkpoint= torch.load(checkpointfile, map_location=device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  last_epoch=(checkpoint['epoch'])\n",
        "  print(\"Remaining Epochs are\",epoch-last_epoch-1)\n",
        "  if(epoch-last_epoch-1<=0):\n",
        "    return\n",
        "  train_simple_network(model,loss_func,train_loader=training_loader, epochs=epoch-last_epoch-1, val_loader = testing_loader,checkpoint_file='new_model_file.pt', \n",
        "                                  score_funcs={'Acc':accuracy_score,'F1': f1_score})\n",
        "\n",
        "resume_simple_network(20,'model.pt')\n",
        "\n",
        "#the resume_simple_network, all the parameters are to be given, and each parameter which is being passed is defined. \n",
        "#the new_model_file.pt stores the file stores the new exection.\n",
        "#the remaining epochs are printed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORBVIZs5WYyR"
      },
      "source": [
        "#reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        " #           https://towardsdatascience.com/checkpointing-deep-learning-models-in-keras-a652570b8de6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T43IjvAvldku"
      },
      "source": [
        "# Question 4\n",
        "The \"deep\" part of deep learning refers to the number of layers in a neural network. Try adding more layers (up to 20) to the models we used for the `make_moons` classification problem. How do more layers impact the performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSBZgNZmmsmE"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=200, noise=0.05)\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi5D1rubnP77"
      },
      "source": [
        "classification_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
        "                                                        torch.tensor(y, dtype=torch.long))\n",
        "training_loader = DataLoader(classification_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_DSKJczvZQm"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlpEYnfsGzot"
      },
      "source": [
        "def visualize2DSoftmax(X, y, model, title=None):\n",
        "    x_min = np.min(X[:,0])-0.5\n",
        "    x_max = np.max(X[:,0])+0.5\n",
        "    y_min = np.min(X[:,1])-0.5\n",
        "    y_max = np.max(X[:,1])+0.5\n",
        "    xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20), np.linspace(y_min, y_max, num=20), indexing='ij')\n",
        "    xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(xy_v, dtype=torch.float32))\n",
        "        y_hat = F.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20), levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)\n",
        "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=cs.ax)\n",
        "    if title is not None:\n",
        "        cs.ax.set_title(title)\n",
        "\n",
        "visualize2DSoftmax(X, y, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOVQSxiLxWAB"
      },
      "source": [
        "#only one layer\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30, 2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoN7vEzuxbDb"
      },
      "source": [
        "results_1=train_simple_network(model, loss_func, training_loader, val_loader=None, score_funcs={'Acc':accuracy_score,'F1': f1_score}, \n",
        "                         epochs=500, device=\"cpu\", checkpoint_file=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPL3U4Tpxem6"
      },
      "source": [
        "def visualize2DSoftmax(X, y, model, title=None):\n",
        "    x_min = np.min(X[:,0])-0.5\n",
        "    x_max = np.max(X[:,0])+0.5\n",
        "    y_min = np.min(X[:,1])-0.5\n",
        "    y_max = np.max(X[:,1])+0.5\n",
        "    xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20), np.linspace(y_min, y_max, num=20), indexing='ij')\n",
        "    xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(xy_v, dtype=torch.float32))\n",
        "        y_hat = F.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20), levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)\n",
        "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=cs.ax)\n",
        "    if title is not None:\n",
        "        cs.ax.set_title(title)\n",
        "\n",
        "visualize2DSoftmax(X, y, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_F5l1yTxfWT"
      },
      "source": [
        "sns.lineplot(x=\"epoch\", y='train Acc', data=results_1, label='Train')\n",
        "#sns.lineplot(x='epoch', y='val Acc', data=results, label='Validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5APWM6xxh8I"
      },
      "source": [
        "sns.lineplot(x='total time', y='train F1', data=results_1, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8R6z1laxl9y"
      },
      "source": [
        "sns.lineplot(x='total time', y='train loss', data=results_1, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulEAJOODleMX"
      },
      "source": [
        "#same neurons and adding more hidden layers\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30,  30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30, 2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL60Kflct8sz"
      },
      "source": [
        "results=train_simple_network(model, loss_func, training_loader, val_loader=None, score_funcs={'Acc':accuracy_score,'F1': f1_score}, \n",
        "                         epochs=500, device=\"cpu\", checkpoint_file=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSDN2lEu_ya"
      },
      "source": [
        "def visualize2DSoftmax(X, y, model, title=None):\n",
        "    x_min = np.min(X[:,0])-0.5\n",
        "    x_max = np.max(X[:,0])+0.5\n",
        "    y_min = np.min(X[:,1])-0.5\n",
        "    y_max = np.max(X[:,1])+0.5\n",
        "    xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20), np.linspace(y_min, y_max, num=20), indexing='ij')\n",
        "    xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(xy_v, dtype=torch.float32))\n",
        "        y_hat = F.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20), levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)\n",
        "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=cs.ax)\n",
        "    if title is not None:\n",
        "        cs.ax.set_title(title)\n",
        "\n",
        "visualize2DSoftmax(X, y, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOiA4lBPmEOD"
      },
      "source": [
        "sns.lineplot(x=\"epoch\", y='train Acc', data=results, label='Train')\n",
        "#sns.lineplot(x='epoch', y='val Acc', data=results, label='Validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Ti8Z8jrOF-"
      },
      "source": [
        "sns.lineplot(x='total time', y='train F1', data=results, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIdkIEE-rpAu"
      },
      "source": [
        "sns.lineplot(x='total time', y='train loss', data=results, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vK07PWujcIQ"
      },
      "source": [
        "Initially without adding layers we observe in the visualization that few (blue)points are in the other region(orange) and vice versa, and few are added in the blank region, which we can state as error region. After adding layers we observe that the model is trained and the activation functions helps in the proper alignment of the points and the regions, we observe that the loss to be reduced. The addition of layers is helpful and adding more than required layers over trains the model and gives us incorrect results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HCL813Gwss6"
      },
      "source": [
        "#Question 5\n",
        "Try changing the number of neurons used in the hidden layers of the `make_moons` classification problem. How does it impact performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaU-4s8tqKJo"
      },
      "source": [
        "#changing the number of neurons\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40,  40),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(40, 2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkhYQ3cQw99C"
      },
      "source": [
        "result = train_simple_network(model, loss_func, training_loader, val_loader=None, score_funcs={'Acc':accuracy_score,'F1': f1_score}, \n",
        "                         epochs=500, device=\"cpu\", checkpoint_file=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZS1oVZcxB8p"
      },
      "source": [
        "def visualize2DSoftmax(X, y, model, title=None):\n",
        "    x_min = np.min(X[:,0])-0.5\n",
        "    x_max = np.max(X[:,0])+0.5\n",
        "    y_min = np.min(X[:,1])-0.5\n",
        "    y_max = np.max(X[:,1])+0.5\n",
        "    xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20), np.linspace(y_min, y_max, num=20), indexing='ij')\n",
        "    xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(xy_v, dtype=torch.float32))\n",
        "        y_hat = F.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20), levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)\n",
        "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=cs.ax)\n",
        "    if title is not None:\n",
        "        cs.ax.set_title(title)\n",
        "\n",
        "visualize2DSoftmax(X, y, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WYOxgJx5WF8"
      },
      "source": [
        " sns.lineplot(x=\"epoch\", y='train Acc', data=result, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhD4323P5PUD"
      },
      "source": [
        "sns.lineplot(x='total time', y='train F1', data=result, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZET2Qjr4cv"
      },
      "source": [
        "sns.lineplot(x='total time', y='train loss', data=result, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luRDcW2axR0C"
      },
      "source": [
        "#increasing the count of the neurons\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50, 2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJtwQxvJxg5X"
      },
      "source": [
        "result1=train_simple_network(model, loss_func, training_loader, val_loader=None, score_funcs={'Acc':accuracy_score,'F1': f1_score}, \n",
        "                         epochs=500, device=\"cpu\", checkpoint_file=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZuafkvyxivD"
      },
      "source": [
        "def visualize2DSoftmax(X, y, model, title=None):\n",
        "    x_min = np.min(X[:,0])-0.5\n",
        "    x_max = np.max(X[:,0])+0.5\n",
        "    y_min = np.min(X[:,1])-0.5\n",
        "    y_max = np.max(X[:,1])+0.5\n",
        "    xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20), np.linspace(y_min, y_max, num=20), indexing='ij')\n",
        "    xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(xy_v, dtype=torch.float32))\n",
        "        y_hat = F.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20), levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)\n",
        "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=cs.ax)\n",
        "    if title is not None:\n",
        "        cs.ax.set_title(title)\n",
        "\n",
        "visualize2DSoftmax(X, y, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvZy2X2xx4Ln"
      },
      "source": [
        "sns.lineplot(x='epoch', y='train Acc', data=result1, label='Train')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7ll5qNR9svL"
      },
      "source": [
        "sns.lineplot(x='total time', y='train F1', data=result1, label='Train')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lQJtpcd9jgY"
      },
      "source": [
        "sns.lineplot(x='total time', y='train loss', data=result1, label='Train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T96VqJyKtJgI"
      },
      "source": [
        "By changing the number of neurons I observe that the error region, i.e., the blank space between the two moons has been reduced. More neurons helps us to reduce more error blank regions in the model. The tilt in the region is also seen clearly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hLLrvl9Jy2o"
      },
      "source": [
        "# Question 6:\n",
        "Use scikit-learn to load the breast cancer wisconsin dataset, and convert it into a `TensorDataset` and then split it into 80% for training and 20% for testing. Try to build your own classification neural network for this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRYn4KOWJz-N"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "\n",
        "#loading the breast cancer data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMI-H-LdL_CG"
      },
      "source": [
        "X=data.data\n",
        "y=data.target\n",
        "\n",
        "#initializing features and Labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFYLvrXmYuWO"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8eJPvasYxR5"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFhzopI_VQZq"
      },
      "source": [
        "data_torch = TensorDataset(torch.tensor(X,dtype=torch.float32),torch.tensor(y, dtype=torch.float32))\n",
        "\n",
        "#converting the data to TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtCIfVurWBf4"
      },
      "source": [
        "data_torch[:][0]\n",
        "\n",
        "#feature values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh1vQnG5M-BR"
      },
      "source": [
        "data_torch[:][1]\n",
        "\n",
        "#target values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cWV5SH4VsOK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#import train_test_split from sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcYpFAGcV1Nl"
      },
      "source": [
        "X_train,X_test,y_train, y_test=train_test_split(data_torch[:][0],data_torch[:][1], test_size=0.2, random_state=3)\n",
        "\n",
        "# spliting the data into 80% for training and 20% for testing and giving random_state 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NmLAQ5sWZus"
      },
      "source": [
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "training_loader = DataLoader(train_dataset, shuffle=True)\n",
        "testing_loader = DataLoader(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATGFkY61KUze"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(30,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50,  50),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(50, 2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcIJsB-uXH5C"
      },
      "source": [
        "results_pd = train_simple_network(model, loss_func, training_loader, epochs=5, \n",
        "                                  val_loader=testing_loader, \n",
        "                                  checkpoint_file='model.pt', \n",
        "                                  score_funcs={'Acc':accuracy_score,'F1': f1_score})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMiQYx45M5Bl"
      },
      "source": [
        "results_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcfdNQCcuPCy"
      },
      "source": [
        "sns.lineplot(x='total time', y='train loss', data=results_pd, label='Train')\n",
        "sns.lineplot(x='total time', y='val loss', data=results_pd, label='Validation')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeFgfUcEsdkV"
      },
      "source": [
        "Other References:\n",
        "\n",
        "1) https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
        "\n",
        "2)https://towardsdatascience.com/checkpointing-deep-learning-models-in-keras-a652570b8de6\n",
        "\n",
        "3)https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
        "\n",
        "4)https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n"
      ]
    }
  ]
}